{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575a18cc-e747-4903-847c-061bfb4e77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific packages\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, AutoModelForCTC, Wav2Vec2ProcessorWithLM\n",
    "from datasets import load_dataset, Audio, load_metric\n",
    "\n",
    "# General packages\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f97e7c3-61e5-4c8b-8742-70ee57428232",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = Path(\"/home/user/code/sbb_project\")\n",
    "DATA_DIR = MAIN_DIR.joinpath(\"data_sbb\")\n",
    "TEST_DATA_DIR = DATA_DIR.joinpath(\"zeroshot_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6490e2-55f4-4df4-8979-d09c02bf62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(TEST_DATA_DIR, '*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48cbbdcb-f366-4dc8-bc46-145e756c7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = \"gleis alpha vier vier via gleis beta vier fÃ¼nf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d4a11f-f9e1-4086-b0b4-22b36e091cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCTC.from_pretrained(\"fxtentacle/wav2vec2-xls-r-1b-tevr\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2db3fad-a879-47b8-8bc3-8dfe99af487c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "class HajoProcessor(Wav2Vec2ProcessorWithLM):\n",
    "    @staticmethod\n",
    "    def get_missing_alphabet_tokens(decoder, tokenizer):\n",
    "        return []\n",
    "processor = HajoProcessor.from_pretrained(\"fxtentacle/wav2vec2-xls-r-1b-tevr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af995e9e-9efc-4a75-908e-e135deac7821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_audio(batch, image=False):    \n",
    "    audio = batch['audio']['array']\n",
    "    # resample, if needed\n",
    "    if batch['audio']['sampling_rate'] != 16000:\n",
    "        audio = T.Resample(orig_freq=batch['audio']['sampling_rate'], new_freq=16000)(torch.from_numpy(audio)).numpy()\n",
    "    # normalize\n",
    "    audio = (audio - audio.mean()) / np.sqrt(audio.var() + 1e-7)\n",
    "    # ask HF processor to prepare audio for GPU eval\n",
    "    input_values = processor(audio, return_tensors=\"pt\", sampling_rate=16_000).input_values\n",
    "    # call model on GPU\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values.to('cuda')).logits.cpu().numpy()[0]\n",
    "    # ask HF processor to decode logits\n",
    "    decoded = processor.decode(logits, beam_width=500)\n",
    "    # return as dictionary\n",
    "    return { 'groundtruth': text_fix(batch['sentence']), 'prediction': decoded.text }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed8cc878-7e9a-484d-9895-8581c0ab7202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import lru_cache\n",
    "from typing import Union\n",
    "\n",
    "import ffmpeg\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def exact_div(x, y):\n",
    "    assert x % y == 0\n",
    "    return x // y\n",
    "\n",
    "# hard-coded audio hyperparameters\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "N_MELS = 80\n",
    "HOP_LENGTH = 160\n",
    "CHUNK_LENGTH = 30\n",
    "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000: number of samples in a chunk\n",
    "N_FRAMES = exact_div(N_SAMPLES, HOP_LENGTH)  # 3000: number of frames in a mel spectrogram input\n",
    "\n",
    "\n",
    "def load_audio(file: str, sr: int = SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Open an audio file and read as mono waveform, resampling as necessary\n",
    "    Parameters\n",
    "    ----------\n",
    "    file: str\n",
    "        The audio file to open\n",
    "    sr: int\n",
    "        The sample rate to resample the audio if necessary\n",
    "    Returns\n",
    "    -------\n",
    "    A NumPy array containing the audio waveform, in float32 dtype.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n",
    "        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n",
    "        out, _ = (\n",
    "            ffmpeg.input(file, threads=0)\n",
    "            .output(\"-\", format=\"s16le\", acodec=\"pcm_s16le\", ac=1, ar=sr)\n",
    "            .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n",
    "        )\n",
    "    except ffmpeg.Error as e:\n",
    "        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
    "\n",
    "    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0\n",
    "\n",
    "\n",
    "def pad_or_trim(array, length: int = N_SAMPLES, *, axis: int = -1):\n",
    "    \"\"\"\n",
    "    Pad or trim the audio array to N_SAMPLES, as expected by the encoder.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(array):\n",
    "        if array.shape[axis] > length:\n",
    "            array = array.index_select(dim=axis, index=torch.arange(length, device=array.device))\n",
    "\n",
    "        if array.shape[axis] < length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, length - array.shape[axis])\n",
    "            array = F.pad(array, [pad for sizes in pad_widths[::-1] for pad in sizes])\n",
    "    else:\n",
    "        if array.shape[axis] > length:\n",
    "            array = array.take(indices=range(length), axis=axis)\n",
    "\n",
    "        if array.shape[axis] < length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, length - array.shape[axis])\n",
    "            array = np.pad(array, pad_widths)\n",
    "\n",
    "    return array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b83df67-2202-47a4-b4b8-6a938710deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(ref, hyp ,debug=True):\n",
    "    r = ref.split()\n",
    "    h = hyp.split()\n",
    "    if (len(h) == 0) or (len(r) == 0):\n",
    "        return {'WER':9, 'numCor':9, 'numSub':9, 'numIns':9, 'numDel':9, \"numCount\": len(r)}\n",
    "    #costs will holds the costs, like in the Levenshtein distance algorithm\n",
    "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "    # backtrace will hold the operations we've done.\n",
    "    # so we could later backtrace, like the WER algorithm requires us to.\n",
    "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    " \n",
    "    OP_OK = 0\n",
    "    OP_SUB = 1\n",
    "    OP_INS = 2\n",
    "    OP_DEL = 3\n",
    "    DEL_PENALTY = 1\n",
    "    INS_PENALTY = 1\n",
    "    SUB_PENALTY = 1\n",
    "    \n",
    "    # First column represents the case where we achieve zero\n",
    "    # hypothesis words by deleting all reference words.\n",
    "    for i in range(1, len(r)+1):\n",
    "        costs[i][0] = DEL_PENALTY*i\n",
    "        backtrace[i][0] = OP_DEL\n",
    "    \n",
    "    # First row represents the case where we achieve the hypothesis\n",
    "    # by inserting all hypothesis words into a zero-length reference.\n",
    "    for j in range(1, len(h) + 1):\n",
    "        costs[0][j] = INS_PENALTY * j\n",
    "        backtrace[0][j] = OP_INS\n",
    "    \n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                costs[i][j] = costs[i-1][j-1]\n",
    "                backtrace[i][j] = OP_OK\n",
    "            else:\n",
    "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
    "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
    "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
    "                 \n",
    "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
    "                if costs[i][j] == substitutionCost:\n",
    "                    backtrace[i][j] = OP_SUB\n",
    "                elif costs[i][j] == insertionCost:\n",
    "                    backtrace[i][j] = OP_INS\n",
    "                else:\n",
    "                    backtrace[i][j] = OP_DEL\n",
    "                 \n",
    "    # back trace though the best route:\n",
    "    i = len(r)\n",
    "    j = len(h)\n",
    "    numSub = 0\n",
    "    numDel = 0\n",
    "    numIns = 0\n",
    "    numCor = 0\n",
    "    if debug:\n",
    "        print(\"OP\\tREF\\tHYP\")\n",
    "        lines = []\n",
    "    while i > 0 or j > 0:\n",
    "        if backtrace[i][j] == OP_OK:\n",
    "            numCor += 1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_SUB:\n",
    "            numSub +=1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_INS:\n",
    "            numIns += 1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
    "        elif backtrace[i][j] == OP_DEL:\n",
    "            numDel += 1\n",
    "            i-=1\n",
    "            if debug:\n",
    "                lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
    "    if debug:\n",
    "        lines = reversed(lines)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "        print(\"#cor \" + str(numCor))\n",
    "        print(\"#sub \" + str(numSub))\n",
    "        print(\"#del \" + str(numDel))\n",
    "        print(\"#ins \" + str(numIns))\n",
    "    # return (numSub + numDel + numIns) / (float) (len(r))\n",
    "    wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
    "    return {'WER':wer_result, 'numCor':numCor, 'numSub':numSub, 'numIns':numIns, 'numDel':numDel, \"numCount\": len(r)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7dd5dd1-b86c-4dd2-837d-15b8ce640df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = list()\n",
    "for file in files:\n",
    "    audio = load_audio(file)\n",
    "    #audio = pad_or_trim(audio)\n",
    "    audio = (audio - audio.mean()) / np.sqrt(audio.var() + 1e-7)\n",
    "    input_values = processor(audio, return_tensors=\"pt\", sampling_rate=16_000).input_values\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values.to('cuda')).logits.cpu().numpy()[0]\n",
    "    decoded = processor.decode(logits, beam_width=500)\n",
    "    preds.append(decoded.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5b215ac-757a-4f3b-8c88-0c69e38f4d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['was er fÃ¼r', 'was auf vier greis der vierte', 'verfiel vier reiste vier fÃ¼nf', 'kreis aus vier vier reiste vier fÃ¼nf', 'bis auf vier spÃ¤ter vier', 'gleich auf vier vier vier gleis spÃ¤ter vier fÃ¼nf', '', 'gleich vier die rÃ¤der vier fÃ¼nf', 'auf ihr gleich viel', 'bereits vier graf', 'bis auf vier gleise vierte', 'er ist']\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29e0df70-b1d3-47bf-8fb5-f4dc71dbba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "wers = list()\n",
    "for pred in preds:\n",
    "    word_error = wer(pred, ground_truth, debug=False)\n",
    "    wers.append(word_error[\"WER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e35704ca-7f44-4c3e-8724-48392802846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER for all sentences: \n",
      " [3.0, 1.333, 1.2, 0.714, 1.4, 0.444, 9, 1.0, 2.25, 2.667, 1.6, 4.5]\n",
      "Average WER: 2.425666666666667 \n",
      " Minimum WER: 0.444 \n",
      " Maximum WER: 9.0\n"
     ]
    }
   ],
   "source": [
    "print(\"WER for all sentences: \\n\", wers)\n",
    "print(\"Average WER: {} \\n Minimum WER: {} \\n Maximum WER: {}\".format(np.mean(wers), np.min(wers), np.max(wers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033a01a-4a9f-4be8-b52a-87ffff64abcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b6c7e6-ee2f-464c-a148-5b058fede5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

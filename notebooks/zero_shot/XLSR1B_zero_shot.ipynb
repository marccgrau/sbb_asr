{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575a18cc-e747-4903-847c-061bfb4e77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific packages\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, AutoModelForCTC, Wav2Vec2ProcessorWithLM\n",
    "from datasets import load_dataset, Audio, load_metric\n",
    "\n",
    "# General packages\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f97e7c3-61e5-4c8b-8742-70ee57428232",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = Path(\"/home/user/code/sbb_project\")\n",
    "DATA_DIR = MAIN_DIR.joinpath(\"data_sbb\")\n",
    "TEST_DATA_DIR = DATA_DIR.joinpath(\"zeroshot_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6490e2-55f4-4df4-8979-d09c02bf62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(TEST_DATA_DIR, '*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48cbbdcb-f366-4dc8-bc46-145e756c7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = \"gleis alpha vier vier via gleis beta vier fünf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d4a11f-f9e1-4086-b0b4-22b36e091cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCTC.from_pretrained(\"fxtentacle/wav2vec2-xls-r-1b-tevr\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2db3fad-a879-47b8-8bc3-8dfe99af487c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "class HajoProcessor(Wav2Vec2ProcessorWithLM):\n",
    "    @staticmethod\n",
    "    def get_missing_alphabet_tokens(decoder, tokenizer):\n",
    "        return []\n",
    "processor = HajoProcessor.from_pretrained(\"fxtentacle/wav2vec2-xls-r-1b-tevr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af995e9e-9efc-4a75-908e-e135deac7821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_audio(batch, image=False):    \n",
    "    audio = batch['audio']['array']\n",
    "    # resample, if needed\n",
    "    if batch['audio']['sampling_rate'] != 16000:\n",
    "        audio = T.Resample(orig_freq=batch['audio']['sampling_rate'], new_freq=16000)(torch.from_numpy(audio)).numpy()\n",
    "    # normalize\n",
    "    audio = (audio - audio.mean()) / np.sqrt(audio.var() + 1e-7)\n",
    "    # ask HF processor to prepare audio for GPU eval\n",
    "    input_values = processor(audio, return_tensors=\"pt\", sampling_rate=16_000).input_values\n",
    "    # call model on GPU\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values.to('cuda')).logits.cpu().numpy()[0]\n",
    "    # ask HF processor to decode logits\n",
    "    decoded = processor.decode(logits, beam_width=500)\n",
    "    # return as dictionary\n",
    "    return { 'groundtruth': text_fix(batch['sentence']), 'prediction': decoded.text }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed8cc878-7e9a-484d-9895-8581c0ab7202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import lru_cache\n",
    "from typing import Union\n",
    "\n",
    "import ffmpeg\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def exact_div(x, y):\n",
    "    assert x % y == 0\n",
    "    return x // y\n",
    "\n",
    "# hard-coded audio hyperparameters\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "N_MELS = 80\n",
    "HOP_LENGTH = 160\n",
    "CHUNK_LENGTH = 30\n",
    "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000: number of samples in a chunk\n",
    "N_FRAMES = exact_div(N_SAMPLES, HOP_LENGTH)  # 3000: number of frames in a mel spectrogram input\n",
    "\n",
    "\n",
    "def load_audio(file: str, sr: int = SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Open an audio file and read as mono waveform, resampling as necessary\n",
    "    Parameters\n",
    "    ----------\n",
    "    file: str\n",
    "        The audio file to open\n",
    "    sr: int\n",
    "        The sample rate to resample the audio if necessary\n",
    "    Returns\n",
    "    -------\n",
    "    A NumPy array containing the audio waveform, in float32 dtype.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n",
    "        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n",
    "        out, _ = (\n",
    "            ffmpeg.input(file, threads=0)\n",
    "            .output(\"-\", format=\"s16le\", acodec=\"pcm_s16le\", ac=1, ar=sr)\n",
    "            .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n",
    "        )\n",
    "    except ffmpeg.Error as e:\n",
    "        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
    "\n",
    "    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0\n",
    "\n",
    "\n",
    "def pad_or_trim(array, length: int = N_SAMPLES, *, axis: int = -1):\n",
    "    \"\"\"\n",
    "    Pad or trim the audio array to N_SAMPLES, as expected by the encoder.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(array):\n",
    "        if array.shape[axis] > length:\n",
    "            array = array.index_select(dim=axis, index=torch.arange(length, device=array.device))\n",
    "\n",
    "        if array.shape[axis] < length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, length - array.shape[axis])\n",
    "            array = F.pad(array, [pad for sizes in pad_widths[::-1] for pad in sizes])\n",
    "    else:\n",
    "        if array.shape[axis] > length:\n",
    "            array = array.take(indices=range(length), axis=axis)\n",
    "\n",
    "        if array.shape[axis] < length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, length - array.shape[axis])\n",
    "            array = np.pad(array, pad_widths)\n",
    "\n",
    "    return array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b83df67-2202-47a4-b4b8-6a938710deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(ref, hyp ,debug=True):\n",
    "    r = ref.split()\n",
    "    h = hyp.split()\n",
    "    if (len(h) == 0) or (len(r) == 0):\n",
    "        return {'WER':9, 'numCor':9, 'numSub':9, 'numIns':9, 'numDel':9, \"numCount\": len(r)}\n",
    "    #costs will holds the costs, like in the Levenshtein distance algorithm\n",
    "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "    # backtrace will hold the operations we've done.\n",
    "    # so we could later backtrace, like the WER algorithm requires us to.\n",
    "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    " \n",
    "    OP_OK = 0\n",
    "    OP_SUB = 1\n",
    "    OP_INS = 2\n",
    "    OP_DEL = 3\n",
    "    DEL_PENALTY = 1\n",
    "    INS_PENALTY = 1\n",
    "    SUB_PENALTY = 1\n",
    "    \n",
    "    # First column represents the case where we achieve zero\n",
    "    # hypothesis words by deleting all reference words.\n",
    "    for i in range(1, len(r)+1):\n",
    "        costs[i][0] = DEL_PENALTY*i\n",
    "        backtrace[i][0] = OP_DEL\n",
    "    \n",
    "    # First row represents the case where we achieve the hypothesis\n",
    "    # by inserting all hypothesis words into a zero-length reference.\n",
    "    for j in range(1, len(h) + 1):\n",
    "        costs[0][j] = INS_PENALTY * j\n",
    "        backtrace[0][j] = OP_INS\n",
    "    \n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                costs[i][j] = costs[i-1][j-1]\n",
    "                backtrace[i][j] = OP_OK\n",
    "            else:\n",
    "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
    "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
    "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
    "                 \n",
    "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
    "                if costs[i][j] == substitutionCost:\n",
    "                    backtrace[i][j] = OP_SUB\n",
    "                elif costs[i][j] == insertionCost:\n",
    "                    backtrace[i][j] = OP_INS\n",
    "                else:\n",
    "                    backtrace[i][j] = OP_DEL\n",
    "                 \n",
    "    # back trace though the best route:\n",
    "    i = len(r)\n",
    "    j = len(h)\n",
    "    numSub = 0\n",
    "    numDel = 0\n",
    "    numIns = 0\n",
    "    numCor = 0\n",
    "    if debug:\n",
    "        print(\"OP\\tREF\\tHYP\")\n",
    "        lines = []\n",
    "    while i > 0 or j > 0:\n",
    "        if backtrace[i][j] == OP_OK:\n",
    "            numCor += 1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_SUB:\n",
    "            numSub +=1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_INS:\n",
    "            numIns += 1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
    "        elif backtrace[i][j] == OP_DEL:\n",
    "            numDel += 1\n",
    "            i-=1\n",
    "            if debug:\n",
    "                lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
    "    if debug:\n",
    "        lines = reversed(lines)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "        print(\"#cor \" + str(numCor))\n",
    "        print(\"#sub \" + str(numSub))\n",
    "        print(\"#del \" + str(numDel))\n",
    "        print(\"#ins \" + str(numIns))\n",
    "    # return (numSub + numDel + numIns) / (float) (len(r))\n",
    "    wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
    "    return {'WER':wer_result, 'numCor':numCor, 'numSub':numSub, 'numIns':numIns, 'numDel':numDel, \"numCount\": len(r)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7dd5dd1-b86c-4dd2-837d-15b8ce640df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = list()\n",
    "for file in files:\n",
    "    audio = load_audio(file)\n",
    "    #audio = pad_or_trim(audio)\n",
    "    audio = (audio - audio.mean()) / np.sqrt(audio.var() + 1e-7)\n",
    "    input_values = processor(audio, return_tensors=\"pt\", sampling_rate=16_000).input_values\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values.to('cuda')).logits.cpu().numpy()[0]\n",
    "    decoded = processor.decode(logits, beam_width=500)\n",
    "    preds.append(decoded.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5b215ac-757a-4f3b-8c88-0c69e38f4d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['was er für', 'was auf vier greis der vierte', 'verfiel vier reiste vier fünf', 'kreis aus vier vier reiste vier fünf', 'bis auf vier später vier', 'gleich auf vier vier vier gleis später vier fünf', '', 'gleich vier die räder vier fünf', 'auf ihr gleich viel', 'bereits vier graf', 'bis auf vier gleise vierte', 'er ist']\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29e0df70-b1d3-47bf-8fb5-f4dc71dbba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "wers = list()\n",
    "for pred in preds:\n",
    "    word_error = wer(pred, ground_truth, debug=False)\n",
    "    wers.append(word_error[\"WER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e35704ca-7f44-4c3e-8724-48392802846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER for all sentences: \n",
      " [3.0, 1.333, 1.2, 0.714, 1.4, 0.444, 9, 1.0, 2.25, 2.667, 1.6, 4.5]\n",
      "Average WER: 2.425666666666667 \n",
      " Minimum WER: 0.444 \n",
      " Maximum WER: 9.0\n"
     ]
    }
   ],
   "source": [
    "print(\"WER for all sentences: \\n\", wers)\n",
    "print(\"Average WER: {} \\n Minimum WER: {} \\n Maximum WER: {}\".format(np.mean(wers), np.min(wers), np.max(wers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033a01a-4a9f-4be8-b52a-87ffff64abcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b6c7e6-ee2f-464c-a148-5b058fede5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
